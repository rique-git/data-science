{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A269YrFQuMk4"
      },
      "source": [
        "# **Challenge Overview – CV Analyzer Agent**\n",
        "\n",
        "In this challenge, your team will support EY's recruitment team by creating an AI-powered tool to improve how they identify, evaluate, and match candidates to five key job openings. Your solution should enhance decision-making in talent acquisition—whether by recommending strong candidates, generating interview questions, or improving the overall process. The goal is to show how AI can boost recruitment quality, reduce time-to-hire, and create real business value.\n",
        "<br>\n",
        "<br>\n",
        "### **Tech & Tools**\n",
        "\n",
        "It is mandatory to develop the solution in **Google Colab** using **Python**.\n",
        "\n",
        "Other than that, you are completely free to choose your own:\n",
        "\n",
        "•\tLibraries and packages: Use any tool you need (e.g., Pandas, Scikit-learn, LangChain, etc.)\n",
        "\n",
        "•\tVisualization tools: Python-based tools (Matplotlib, Seaborn), Power BI, Tableau, etc. (if you use external visualization tools, don't forget to include prints in the submission zip folder)\n",
        "\n",
        "•\tAI assistants: Feel free to consult ChatGPT, GitHub Copilot, Gemini, or any other.\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "### **Tech Configuration**\n",
        "\n",
        "We suggest using either Gemini or LLaMA as your LLM. Consider your solution’s priorities: Gemini is faster but has context limitations, while LLaMA can handle more context but may be slower.\n",
        "\n",
        "Feel free to use, adapt and complement the functions as needed, or redo them completely to your specifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gB3yL0i7RZBA"
      },
      "outputs": [],
      "source": [
        "def config_llm_gemini(temperature:int):\n",
        "  '''LLM api calling using Gemini  '''\n",
        "  # Steps for students:\n",
        "  # - Go to https://aistudio.google.com/app/apikey and generate your Gemini API key.\n",
        "  # - Add the necessary packages to your requirements.txt:\n",
        "  #    langchain\n",
        "  #    langchain-google-genai\n",
        "  # - Run the following command to install them:\n",
        "  #     !pip install -r requirements.txt\n",
        "  # - Follow the official integration guide for LangChain + Google Generative AI:\n",
        "  #     https://python.langchain.com/docs/integrations/chat/google_generative_ai/\n",
        "  # Pay attention to the request limits of the chosen model.\n",
        "  return \"llm\" #Should return the LLM response"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
